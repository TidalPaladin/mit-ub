{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Atomic Add\n",
    "\n",
    "Below is a test of Triton's atomic add functionality. We'll compare two different approaches for atomic addition:\n",
    "1. `tl.atomic_add` - Atomic addition\n",
    "2. `tl.atomic_cas` - Atomic compare and swap guard\n",
    "\n",
    "First we start with a non-atomic operation. We will produce a vector of values and `tl.store` them to the same address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Minimal\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 1 / 1 (100.0%)\nGreatest absolute difference: 0.38307708501815796 at index (0,) (up to 1e-05 allowed)\nGreatest relative difference: 0.2929307222366333 at index (0,) (up to 1.3e-06 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mAssertionError\u001b[0m\u001b[0;31m:\u001b[0m Tensor-likes are not close!\n\nMismatched elements: 1 / 1 (100.0%)\nGreatest absolute difference: 0.38307708501815796 at index (0,) (up to 1e-05 allowed)\nGreatest relative difference: 0.2929307222366333 at index (0,) (up to 1.3e-06 allowed)\n"
     ]
    }
   ],
   "source": [
    "%xmode Minimal\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "from torch.testing import assert_close\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def kernel_non_atomic(x_p, o_p, BLOCK: tl.constexpr):\n",
    "    # Load x\n",
    "    ptrs = tl.arange(0, BLOCK)\n",
    "    x = tl.load(x_p + ptrs)\n",
    "    # Store all values of vector x into single scalar o\n",
    "    ptrs = tl.zeros_like(ptrs)\n",
    "    tl.store(o_p + ptrs, x)\n",
    "\n",
    "\n",
    "def non_atomic():\n",
    "    torch.random.manual_seed(0)\n",
    "    BLOCK = 32\n",
    "    x = torch.randn(BLOCK, device=\"cuda\")\n",
    "    o = x.new_zeros(1)\n",
    "    kernel_non_atomic[(1,)](x, o, BLOCK)\n",
    "    exp = x.sum()\n",
    "    assert_close(o.view(1), exp.view(1))\n",
    "\n",
    "\n",
    "non_atomic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 2:69:def kernel(a_p, b_p, o_p, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr):\n    a = tl.load(a_p + tl.arange(0, BLOCK_M) * BLOCK_K + tl.arange(0, BLOCK_K))\n                                                                     ^\nValueError('Cannot make_shape_compatible: incompatible dimensions at index 0: 32 and 16')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mValueError\u001b[0m\u001b[0;31m:\u001b[0m Cannot make_shape_compatible: incompatible dimensions at index 0: 32 and 16\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCompilationError\u001b[0m\u001b[0;31m:\u001b[0m at 2:69:def kernel(a_p, b_p, o_p, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr):\n    a = tl.load(a_p + tl.arange(0, BLOCK_M) * BLOCK_K + tl.arange(0, BLOCK_K))\n                                                                     ^\nValueError('Cannot make_shape_compatible: incompatible dimensions at index 0: 32 and 16')\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def kernel(a_p, b_p, o_p, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr):\n",
    "    a = tl.load(a_p + tl.arange(0, BLOCK_M)[:, None] * BLOCK_K + tl.arange(0, BLOCK_K)[None, :])\n",
    "    b = tl.load(b_p + tl.arange(0, BLOCK_N)[:, None] * BLOCK_K + tl.arange(0, BLOCK_K)[None, :])\n",
    "    diff = a[:, None, :] - b[None, :, :]\n",
    "    diff *= diff\n",
    "    diff = tl.sum(diff, 2)\n",
    "    tl.store(o_p + tl.arange(0, BLOCK_M)[:, None] * BLOCK_N + tl.arange(0, BLOCK_N)[None, :], diff)\n",
    "\n",
    "def launch():\n",
    "    M, N, K = 32, 32, 16\n",
    "    a = torch.randn(M, K, device=\"cuda\")\n",
    "    b = torch.randn(N, K, device=\"cuda\")\n",
    "    o = a.new_zeros(M, N)\n",
    "    kernel[(1,)](a, b, o, M, N, K)\n",
    "    exp = torch.cdist(a, b)\n",
    "    assert_close(o, exp)\n",
    "\n",
    "launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `tl.store` does not use atomic addition when storing a vector of values into a single address. Nor will it atomically add across parallel programs.\n",
    "\n",
    "Now let's try atomic addition. We know this will atomically add across parallel programs, but how does it behave when we have one program writing to the same address?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def kernel_atomic(x_p, o_p, BLOCK: tl.constexpr):\n",
    "    # Load x\n",
    "    ptrs = tl.arange(0, BLOCK)\n",
    "    x = tl.load(x_p + ptrs)\n",
    "    # Store all values of vector x into single scalar o\n",
    "    ptrs = tl.zeros_like(ptrs)\n",
    "    tl.atomic_add(o_p + ptrs, x)\n",
    "\n",
    "\n",
    "def atomic_add():\n",
    "    torch.random.manual_seed(0)\n",
    "    BLOCK = 32\n",
    "    x = torch.randn(BLOCK, device=\"cuda\")\n",
    "    o = x.new_zeros(1)\n",
    "    kernel_atomic[(1,)](x, o, BLOCK)\n",
    "    exp = x.sum()\n",
    "    assert_close(o.view(1), exp.view(1))\n",
    "\n",
    "\n",
    "atomic_add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tl.atomic_add` properly accumulates the values of the vector `x` into the scalar `o`. Meaning it is atomic not just across parallel calls, but also across repeated pointers within a single program.\n",
    "\n",
    "Now let's compare the runtime of `tl.atomic_add` and `tl.atomic_cas`. For this example we'll assume that the program has no duplicate addresses within a single program, i.e. we only need atomicity across parallel programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing atomic_add\n",
      "Testing atomic_cas\n",
      "Atomic add took 0.45505693554878235 ms\n",
      "Atomic cas took 2.5705363750457764 ms\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def kernel_add(x_p, o_p, BLOCK: tl.constexpr):\n",
    "    # Load x\n",
    "    ptrs = tl.arange(0, BLOCK)[:, None] * BLOCK + tl.arange(0, BLOCK)[None, :]\n",
    "    x = tl.load(x_p + ptrs)\n",
    "    # Store x into o\n",
    "    tl.atomic_add(o_p + ptrs, x)\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def kernel_cas(x_p, o_p, lock_p, BLOCK: tl.constexpr):\n",
    "    # Load x\n",
    "    ptrs = tl.arange(0, BLOCK)[None, :] * BLOCK + tl.arange(0, BLOCK)[:, None]\n",
    "    x = tl.load(x_p + ptrs)\n",
    "\n",
    "    # Acquire lock\n",
    "    while tl.atomic_cas(lock_p, 0, 1) != 0:\n",
    "        pass\n",
    "    x += tl.load(o_p + ptrs)\n",
    "    tl.store(o_p + ptrs, x)\n",
    "    tl.atomic_xchg(lock_p, 0)\n",
    "\n",
    "\n",
    "def launch(method, dtype=torch.float32, check=True):\n",
    "    torch.random.manual_seed(0)\n",
    "    BLOCK = 64\n",
    "    GRID = 2048\n",
    "    x = torch.randn(BLOCK, BLOCK, device=\"cuda\", dtype=dtype)\n",
    "    o = torch.zeros_like(x)\n",
    "\n",
    "    if method == \"add\":\n",
    "        kernel = kernel_add[(GRID,)](x, o, BLOCK, num_warps=4)\n",
    "    else:\n",
    "        lock = torch.zeros(1, device=\"cuda\", dtype=torch.int32)\n",
    "        kernel = kernel_cas[(GRID,)](x, o, lock, BLOCK, num_warps=4)\n",
    "    if check:\n",
    "        exp = x * GRID\n",
    "        assert_close(o, exp, rtol=1e-4, atol=0)\n",
    "    return kernel\n",
    "\n",
    "print(\"Testing atomic_add\")\n",
    "launch(\"add\")\n",
    "print(\"Testing atomic_cas\")\n",
    "launch(\"cas\")\n",
    "\n",
    "add_ms = triton.testing.do_bench(lambda: launch(\"add\"))\n",
    "cas_ms = triton.testing.do_bench(lambda: launch(\"cas\"))\n",
    "print(f\"Atomic add took {add_ms} ms\")\n",
    "print(f\"Atomic cas took {cas_ms} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some limitations to `tl.atomic_add`. Notably it does not support `tl.bfloat16`. It appears that bfloat16 support only exists on SM_90 (Hopper) and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "from torch.testing import assert_close\n",
    "\n",
    "@triton.jit\n",
    "def kernel(\n",
    "    x_p, y_p, o_p, lock_p,\n",
    "    M, N, K,\n",
    "    stride_x_b, stride_x_m, stride_x_k,\n",
    "    stride_y_b, stride_y_n, stride_y_k,\n",
    "    stride_o_b, stride_o_m, stride_o_n,\n",
    "    stride_lock_b,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "    ATOMIC_ADD: tl.constexpr = True,\n",
    "):\n",
    "    b = tl.program_id(0)\n",
    "    start_m = tl.program_id(1) * BLOCK_M\n",
    "\n",
    "    x_p += b * stride_x_b \n",
    "    y_p += b * stride_y_b \n",
    "    o_p += b * stride_o_b\n",
    "    lock_p += b * stride_lock_b\n",
    "\n",
    "    X_ptr = tl.make_block_ptr(x_p, (M, K), (stride_x_m, stride_x_k), (start_m, 0), (BLOCK_M, BLOCK_K), (1, 0))\n",
    "    x = tl.load(X_ptr)\n",
    "\n",
    "    Y_ptr = tl.make_block_ptr(y_p, (N, K), (stride_y_n, stride_y_k), (0, 0), (BLOCK_N, BLOCK_K), (1, 0))\n",
    "    for _ in range(0, N, BLOCK_N):\n",
    "        y = tl.load(Y_ptr)\n",
    "        o = tl.dot(x, tl.trans(y))\n",
    "        offsets = tl.arange(0, BLOCK_M)[:, None] * stride_o_m + tl.arange(0, BLOCK_N)[None, :] * stride_o_n\n",
    "        if ATOMIC_ADD:\n",
    "            tl.atomic_add(o_p + offsets, o)\n",
    "        else:\n",
    "            while tl.atomic_cas(lock_p, 0, 1) != 0:\n",
    "                pass\n",
    "            o += tl.load(o_p + offsets)\n",
    "            tl.store(o_p + offsets, o)\n",
    "            tl.atomic_xchg(lock_p, 0)\n",
    "            lock_p += 1\n",
    "        o_p += BLOCK_N * stride_o_n\n",
    "\n",
    "\n",
    "def launch(method, dtype=torch.float32):\n",
    "    torch.random.manual_seed(0)\n",
    "    B, M, N, K = 4, 512, 512, 64\n",
    "    x = torch.randn(B, M, K, device=\"cuda\", dtype=dtype)\n",
    "    y = torch.randn(B, N, K, device=\"cuda\", dtype=dtype)\n",
    "    o = x.new_zeros(B, M, N)\n",
    "\n",
    "    if method == \"cas\":\n",
    "        lock = torch.zeros(B, M, device=\"cuda\", dtype=torch.int32)\n",
    "    else:\n",
    "        lock = torch.empty(1, device=\"cuda\", dtype=torch.int32)\n",
    "\n",
    "    def grid(META):\n",
    "        return (B, triton.cdiv(M, META[\"BLOCK_M\"]))\n",
    "\n",
    "    return kernel[grid](\n",
    "        x, y, o, lock,\n",
    "        M, N, K,\n",
    "        x.stride(0), x.stride(1), x.stride(2),\n",
    "        y.stride(0), y.stride(1), y.stride(2),\n",
    "        o.stride(0), o.stride(1), o.stride(2),\n",
    "        lock.stride(0),\n",
    "        BLOCK_M=64, BLOCK_N=64, BLOCK_K=K, \n",
    "        ATOMIC_ADD=(method == \"add\"),\n",
    "        num_warps=4,\n",
    "    )\n",
    "\n",
    "cas_ms = triton.testing.do_bench(lambda: launch(\"cas\"))\n",
    "add_ms = triton.testing.do_bench(lambda: launch(\"add\"))\n",
    "print(f\"Atomic add took {add_ms} ms\")\n",
    "print(f\"Atomic cas took {cas_ms} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mit-ub-7pzcQwz--mit_ub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
