# Config for distilling a MJEPA trained ViT into a ConvNext model.
# See 'mjepa.yaml' for explanation of the config.
fit:
  float32_matmul_precision: medium

  trainer:
    accelerator: gpu
    devices: [2]
    precision: "bf16-mixed"
    max_steps: 50000
    num_sanity_val_steps: 2
    check_val_every_n_epoch: 10

    default_root_dir: /mnt/flash/mit-ub

    callbacks:
      # Save the best model based on validation loss and most recent model
      - class_path: pytorch_lightning.callbacks.ModelCheckpoint
        init_args:
          filename: "epoch={epoch}-step={step}-loss={val/loss:.4f}"
          monitor: "val/loss"
          auto_insert_metric_name: false
          mode: min
          save_last: true

      # Log learning rate and momentum
      - class_path: pytorch_lightning.callbacks.LearningRateMonitor
        init_args:
          log_momentum: true

    logger: 
      class_path: pytorch_lightning.loggers.wandb.WandbLogger
      init_args:
        save_dir: ${fit.trainer.default_root_dir}
        project: mit-ub

  model:
    # Use the MJEPA task with a linear probe
    class_path: mit_ub.tasks.DistillationWithClassification
    init_args:

      backbone_config:
        class_path: mit_ub.model.ConvNextConfig
        init_args:
          in_channels: 3
          depths: [3, 5, 3]
          up_depths: [3, 5]
          dims: [96, 192, 384]
          dims_feedforward: [384, 768, 1536]
          patch_size: 2
          activation: "relu2"
          gate_activation: null
          bias: false
          dropout: 0.1
          stochastic_depth: 0.1
          kernel_size: 3

      teacher_config:
        class_path: mit_ub.model.ViTConfig
        init_args:
          in_channels: 3
          dim: 384
          dim_feedforward: 1536
          patch_size: [4, 4]
          depth: 12
          nhead: 12
          activation: "relu2"
          gate_activation: null
          bias: false
          dropout: 0.1
          stochastic_depth: 0.1
          layer_scale: 0.1
          qk_norm: false

      teacher_checkpoint: /mnt/flash/mit-ub/mit-ub/uueyg9ip/checkpoints/last.ckpt

      num_classes: 10

      # Partial checkpoints are supported. In non-strict mode we ignore missing keys, unexpected keys, and
      # weights of mismatched shapes. The training pipeline reports the % of weights successfully loaded and
      # the names of unloaded layers.
      strict_checkpoint: false

      # Use AdamW optimizer with 8-bit precision, and a reciplrocal square root learning rate schedule
      optimizer_init:
        class_path: bitsandbytes.optim.AdamW8bit
        init_args:
          lr: 0.0001
          weight_decay: 0.05
          betas: [0.85, 0.999]
      lr_interval: "step"
      lr_scheduler_init:
        class_path: deep_helpers.optim.ReciprocalSquareRootLR
        init_args:
          warmup_steps: 5000
          cooldown_steps: 25000
          total_steps: ${fit.trainer.max_steps}
          timescale: 10000
          initial_lr: 0.00002
          initial_momentum: 0.95

      parameter_groups:
        - params:
            - "classification_head"
          weight_decay: 1.0

  data:
    class_path: mit_ub.data.CIFAR10DataModule
    init_args:
      root: /mnt/data/cifar10/cifar-10-batches-py
      batch_size: 512
      num_workers: 12
      pin_memory: true

      # A minimal set of augmentations are used
      train_transforms:
        class_path: torchvision.transforms.v2.Compose
        init_args:
          transforms:
            - class_path: torchvision.transforms.v2.RandomHorizontalFlip
              init_args:
                p: 0.5

            - class_path: torchvision.transforms.v2.RandomVerticalFlip
              init_args:
                p: 0.5

            - class_path: torchvision.transforms.v2.ColorJitter
              init_args:
                brightness: 0.2
                contrast: 0.2
                saturation: 0.2

            - class_path: torchvision.transforms.v2.ToTensor

      val_transforms:
        class_path: torchvision.transforms.v2.ToTensor
