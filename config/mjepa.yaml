# Config for training a basic ViT using MJEPA on CIFAR-10.
# NOTE: Set environment variable TORCH_COMPILE=0 to disable torch.compile. This is helpful for debugging
# since compilation takes time. However, it does yield a substantial throughput improvement.
fit:
  # Leverage tensor cores for matmul
  float32_matmul_precision: medium

  trainer:
    # Train on 3 GPUs in BF16 mixed precision for 25k steps, validating every 20 epochs.
    # Sanity validation steps are recommended to trigger torch.compile of the model in inference mode.
    accelerator: gpu
    devices: 3
    precision: "bf16-mixed"
    max_steps: 25000
    num_sanity_val_steps: 2
    check_val_every_n_epoch: 20

    gradient_clip_val: 1.0
    gradient_clip_algorithm: norm

    default_root_dir: /mnt/flash/mit-ub

    callbacks:
      # Save the best model based on validation loss and most recent model
      # This custom callback synchronizes the teacher weights before saving the checkpoint,
      # and it ensures that a final checkpoint is saved when fitting is complete.
      - class_path: mit_ub.callbacks.ModelCheckpoint
        init_args:
          filename: "step={step}-acc={val/acc:.4f}"
          monitor: "val/acc"
          auto_insert_metric_name: false
          mode: max
          save_last: true

      # Log learning rate and momentum
      - class_path: pytorch_lightning.callbacks.LearningRateMonitor
        init_args:
          log_momentum: true

    logger: 
      class_path: pytorch_lightning.loggers.wandb.WandbLogger
      init_args:
        save_dir: ${fit.trainer.default_root_dir}
        project: mit-ub
        name: mjepa

  model:
    # Use the MJEPA task with a linear probe
    class_path: mit_ub.tasks.JEPAWithClassification
    init_args:

      # The ViT backbone
      backbone_config:
        class_path: mit_ub.model.ViTConfig
        init_args:
          in_channels: 3
          dim: 384
          dim_feedforward: 1536
          patch_size: [4, 4]
          depth: 18
          nhead: 12
          activation: "relu2"
          gate_activation: null
          bias: false
          dropout: 0.1
          stochastic_depth: 0.25
          layer_scale: 0.1 # Layer scale can stabilize early training
          qk_norm: false

          # This MoE config does comparable to the default config
          #moe_layers: [15]
          #num_experts: 32
          #num_slots: 32

      # Configure the probe head
      classification_config:
        num_classes: 10
        pool_type: simple-attention # Use multi-head attention pooling followed by linear projection to output logits
        mlp_tower: true
        tower_input_norm: false

      #checkpoint: ...

      # Partial checkpoints are supported. In non-strict mode we ignore missing keys, unexpected keys, and
      # weights of mismatched shapes. The training pipeline reports the % of weights successfully loaded and
      # the names of unloaded layers.
      #strict_checkpoint: true

      jepa_config:
        # Reciprocal square root EMA momentum schedule, from `initial_momentum` to `momentum` during warmup,
        # held at `momentum` during the peak steps, and ending at `initial_momentum` after cooldown.
        # If `stopped_steps` is set, the schedule will run at `initial_momentum` for the final `stopped_steps` steps.
        # We match these roughly against the LR schedule as a good initial hyperparameter guess.
        ema_config: 
          momentum: 0.98
          initial_momentum: 1.0
          warmup_steps: 500
          peak_steps: 5000
          cooldown_steps: 2500
          stopped_steps: 5000
          timescale: 2000

        # Optional final weight decay for the backbone
        # NOTE: Using a weight decay schedule breaks the infinite LR schedule provided by the reciprocal square root scheduler.
        weight_decay_final: null

        # Depth of transformer decoder making JEPA predictions.
        # Setting to 1/2 or 1/3 of the backbone depth is a good starting point.
        predictor_depth: 9
        # By default the predictor will not use self-attention. This is empirically comparable to using self-attention.
        self_attn: false

        # Ratios of input to sample as context and targets.
        # Should satisfy context_ratio + target_ratio <= 1.0
        context_ratio: 0.5
        target_ratio: 0.5

        # How much to subsample the encoded context before passing to decoder.
        # context_ratio = 0.5, context_subsample_ratio = 0.5 -> 1/4 of input given to decoder
        # At context_subsample_ratio = 1.0, no subsampling of the context is done.
        # For efficiency, set this such that the subsampled context length is a multiple of 16 (on tensor cores).
        # This should encourage token redundancy on the part of the backbone.
        context_subsample_ratio: 0.75

        # Scale of context sampling. 
        # At scale=2, tokens are guaranteed to appear in 2x2 contiguous groups.
        scale: 2

        # Use a simple linear layer on the JEPA predictor output projection
        mlp_tower: true
        tower_input_norm: false

        # A SigLIP style predictor is used between the student context and teacher output.
        # When this weight is zero, there is a stop gradient applied to the SigLIP pathway.
        # Otherwise, the SigLIP predictor can influence backbone updates.
        siglip_weight: 0.2

      # Use SOAP optimizer and a reciplrocal square root learning rate schedule.
      # SOAP is empirically much faster than AdamW for this task.
      # The reciprocal square root scheduler is an "infinite" schedule in that the schedule can be resumed from
      # an existing checkpoint.
      optimizer_init:
        class_path: pytorch_optimizer.SOAP
        init_args:
          lr: 0.001
          weight_decay: 0.1
          betas: [0.85, 0.999]
          precondition_frequency: 10
      lr_interval: "step"
      lr_scheduler_init:
        class_path: deep_helpers.optim.ReciprocalSquareRootLR
        init_args:
          warmup_steps: 1000
          cooldown_steps: 7500
          total_steps: ${fit.trainer.max_steps}
          timescale: 1000
          initial_lr: 0.0002
          initial_momentum: 0.95
          peak_steps: 5000

      # Override optimizer parameters for special parameter groups
      # NOTE: For parameters that match multiple groups, they are assigned to the first group that matches.
      parameter_groups:
        # Biases (no weight decay)
        - params:
            - "bias"
            - "b_in"
            - "b_out"
            - "b_gate"
            - "b_q"
            - "b_k"
            - "b_v"
            - "b_norm"
            - "b_qk_norm"
            - "siglip_b"
          weight_decay: 0.0
        # LayerScale (no weight decay)
        - params:
            - "LayerScale"
          weight_decay: 0.0
        # Multiplicative weights (no weight decay)
        - params:
            - "LayerNorm"
            - "w_norm"
            - "w_qk_norm"
            - "siglip_t"
          weight_decay: 0.0
        # Linear probe (higher weight decay)
        - params:
            - "jepa_head"
            - "linear_probe"
            - "pool"
          weight_decay: 1.0

  data:
    class_path: mit_ub.data.CIFAR10DataModule
    init_args:
      root: /mnt/data/cifar10/cifar-10-batches-py
      batch_size: 1024
      num_workers: 24 # Recommended setting is (num cpu cores) / (num gpus)
      pin_memory: true

      train_transforms:
        class_path: torchvision.transforms.v2.Compose
        init_args:
          transforms:
            - class_path: torchvision.transforms.v2.RandomHorizontalFlip
              init_args:
                p: 0.5

            - class_path: torchvision.transforms.v2.RandomVerticalFlip
              init_args:
                p: 0.5

            - class_path: torchvision.transforms.v2.RandomResizedCrop
              init_args:
                size: [32, 32]
                scale: [0.5, 1.0]
                ratio: [0.75, 1.33]

            - class_path: torchvision.transforms.v2.RandomApply
              init_args:
                p: 0.25
                transforms:
                  - class_path: mit_ub.data.RandomRotation
                    init_args:
                      degrees: 45

            - class_path: torchvision.transforms.v2.ColorJitter
              init_args:
                brightness: 0.2
                contrast: 0.2
                saturation: 0.2

            - class_path: torchvision.transforms.v2.RandomGrayscale
              init_args:
                p: 0.1

            - class_path: torchvision.transforms.v2.ToImage
            
            - class_path: torchvision.transforms.v2.ToDtype
              init_args:
                scale: true
                dtype:
                  class_path: torch.float32

      val_transforms:
        class_path: torchvision.transforms.v2.Compose
        init_args:
          transforms:
            - class_path: torchvision.transforms.v2.ToImage
            
            - class_path: torchvision.transforms.v2.ToDtype
              init_args:
                scale: true
                dtype:
                  class_path: torch.float32
