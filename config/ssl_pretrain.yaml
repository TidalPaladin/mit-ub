fit:
  compile: true
  float32_matmul_precision: medium
  trainer:
    accelerator: gpu
    devices: 2
    strategy: "ddp_find_unused_parameters_true"
    precision: "bf16-mixed"
    max_steps: 250000
    num_sanity_val_steps: 2
    default_root_dir: /mnt/iscsi/outputs/mit-ub
    gradient_clip_val: 5.0
    gradient_clip_algorithm: "norm"
    check_val_every_n_epoch: 5
    callbacks:
      - class_path: pytorch_lightning.callbacks.ModelCheckpoint
        init_args:
          filename: "epoch={epoch}-step={step}-loss={val/loss:.4f}"
          monitor: "val/loss"
          auto_insert_metric_name: false
          mode: min
          save_last: true

      - class_path: pytorch_lightning.callbacks.LearningRateMonitor
        init_args:
          log_momentum: true

      - class_path: mit_ub.callbacks.LogInputCallback
        init_args:
          name: input
          modes:
            - "TRAIN"
          queue_size: 4
          flush_interval: 1000

      - class_path: mit_ub.callbacks.LogInputCallback
        init_args:
          name: input
          modes:
            - "VAL"
            - "TEST"
          queue_size: 4

      #- class_path: deep_helpers.callbacks.MultiTaskCallbackWrapper
      #  init_args:
      #    wrapped:
      #      class_path: mit_ub.callbacks.LogMAECallback
      #      init_args:
      #        name: input
      #        modes:
      #          - "TRAIN"
      #        queue_size: 2
      #        flush_interval: 1000
      #    tasks:
      #      - "mae"

    logger:
      class_path: pytorch_lightning.loggers.wandb.WandbLogger
      init_args:
        save_dir: /mnt/iscsi/outputs/mit-ub
        project: mit-ub

  #model:
  #  class_path: gpvit.train.tasks.QueryPatch
  #  init_args:
  #    backbone: "gpvit_mlpmixer_base_i512x384_p16"

  model:
    class_path: gpvit.train.tasks.MultiTask
    init_args:
      tasks:
        -
          - "contrastive"
          - class_path: gpvit.train.tasks.ContrastiveEmbedding
            init_args:
              backbone: "gpvit_mlpmixer_base_i512x384_p16"
        -
          - "mae"
          - class_path: gpvit.train.tasks.MAE
            init_args:
              backbone: "gpvit_mlpmixer_base_i512x384_p16"
        -
          - "jepa"
          - class_path: gpvit.train.tasks.JEPA
            init_args:
              backbone: "gpvit_mlpmixer_base_i512x384_p16"
        -
          - "query-patch"
          - class_path: gpvit.train.tasks.QueryPatch
            init_args:
              backbone: "gpvit_mlpmixer_base_i512x384_p16"

      optimizer_init:
        class_path: torch.optim.AdamW
        init_args:
          lr: 0.00003
          weight_decay: 0.05
      weight_decay_exemptions:
        - "bias"
        - "LayerNorm"
        - "position"
      lr_interval: "step"
      lr_scheduler_init:
        class_path: torch.optim.lr_scheduler.OneCycleLR
        init_args:
          max_lr: 0.00001
          div_factor: 1
          final_div_factor: 50
          pct_start: 0.05
          three_phase: false
          total_steps: 250000
      checkpoint: /mnt/iscsi/rsna_final.ckpt
      strict_checkpoint: false

  data:
    class_path: mit_ub.data.ImageDataModule
    init_args:
      train_inputs: 
        - /mnt/data/mammo-pretrain/ddsm
        - /mnt/data/mammo-pretrain/bcsdbt_maxip
        - /mnt/data/mammo-pretrain/bcsdbt_centralslice
        - /mnt/data/mammo-pretrain/mahajan
      val_inputs:
        - /mnt/data/mammo-pretrain/aigora
      batch_size: 32
      num_workers: 20

      train_transforms:
        class_path: torchvision.transforms.v2.Compose
        init_args:
          transforms:
            - class_path: torchvision.transforms.v2.RandomResizedCrop
              init_args:
                size: [512, 384]
                scale: [0.8, 1.0]
                ratio: [0.8, 1.2]
                antialias: true

            - class_path: torchvision.transforms.v2.RandomHorizontalFlip
              init_args:
                p: 0.5

            - class_path: torchvision.transforms.v2.RandomVerticalFlip
              init_args:
                p: 0.5

            - class_path: torchvision.transforms.v2.RandomRotation
              init_args:
                degrees: 20

            - class_path: torchvision.transforms.v2.ColorJitter
              init_args:
                brightness: 0.2
                contrast: 0.2

            - class_path: torchvision.transforms.v2.RandomInvert
              init_args:
                p: 0.5

      #val_transforms:
      #  class_path: torchvision.transforms.v2.Compose
      #  init_args:
      #    transforms:
      #      - class_path: torchvision.transforms.v2.Resize
      #        init_args:
      #          size: 256
      #          antialias: true